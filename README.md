### Hi there, I'm Meng Yi (å­Ÿæ¯…) ğŸ‘‹

<div align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=0:000000,100:303030&height=200&section=header&text=Logic%20Meets%20AI&fontSize=70&fontColor=ffffff&desc=Master%20Student%20@%20SYSU%20%7C%20Neuro-Symbolic%20AI%20Enthusiast&descSize=20&fontAlignY=40" width="100%"/>
</div>

---

### ğŸš€ About Me / å…³äºæˆ‘

I am a Master's student in **Logic** at **Sun Yat-sen University**, exploring **Neuro-Symbolic AI**.
æˆ‘æ˜¯**ä¸­å±±å¤§å­¦é€»è¾‘å­¦ç¡•å£«**ï¼Œè‡´åŠ›äºæ¢ç´¢**é€»è¾‘æ¨ç†ä¸æ·±åº¦å­¦ä¹ **çš„ç»“åˆã€‚

- ğŸ”­ **Current Focus:** LLM Reasoning (CoT/ToT), Parameter-Efficient Fine-Tuning (QLoRA).
  <br>*(ä¸»æ”»æ–¹å‘ï¼šå¤§æ¨¡å‹æ¨ç†é“¾ã€å‚æ•°é«˜æ•ˆå¾®è°ƒ)*
- ğŸ§  **Research Interest:** **AI Alignment** & **Logical Consistency**.
  <br>*(ç ”ç©¶å…´è¶£ï¼šAIå¯¹é½ã€é€»è¾‘ä¸€è‡´æ€§ã€åäº‹å®æ¨ç†)*
- ğŸ› ï¸ **Tech Stack:** PyTorch, Hugging Face, Linux (WSL2), Docker.

---

### ğŸ› ï¸ Tech Stack & Tools

<p align="left">
  <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white" />
  <img src="https://img.shields.io/badge/Hugging%20Face-FFD21E?style=for-the-badge&logo=huggingface&logoColor=black" />
  <img src="https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black" />
  <img src="https://img.shields.io/badge/WandB-FFBE00?style=for-the-badge&logo=weightsandbiases&logoColor=black" />
</p>

---

### ğŸ“š Paper Reading / è®ºæ–‡å¤ç°ç¬”è®°

### Phase 1: The Foundation (BERT & GPT)
| Date | Paper Title | Links | Tags |
| :--- | :--- | :--- | :--- |
| 2025-12-30 | **Attention Is All You Need** | [PDF](https://arxiv.org/pdf/1706.03762.pdf) | `Transformer` | 
| 2025-12-30 | **BERT: Pre-training of Deep Bidirectional Transformers** | [PDF](https://arxiv.org/pdf/1810.04805.pdf) | `Encoder` |
| 2026-01-08 | **Language Models are Few-Shot Learners (GPT-3)** | [PDF](https://arxiv.org/pdf/2005.14165.pdf) | `Decoder` |

### Phase 2: Reasoning & Agents
| Date | Paper Title | Links | Tags |
| :--- | :--- | :--- | :--- |
| 2026-01-09 | **Chain-of-Thought Prompting Elicits Reasoning** | [PDF](https://arxiv.org/pdf/2201.11903.pdf) | `CoT` |
| 2026-01-10 | **Self-Consistency Improves Chain of Thought Reasoning in Language Models** | [PDF](https://arxiv.org/abs/2203.11171) | `CoT-SC` |
| 2026-01-13 | **Tree of Thoughts: Deliberate Problem Solving with Large Language Models** | [PDF](https://arxiv.org/abs/2305.10601) | `ToT` |
| 2026-01-14 | **ReAct: Synergizing Reasoning and Acting in Language Models** | [PDF](https://arxiv.org/abs/2210.03629) | `Agentã€tools_call` |

### Phase 3: Fine_tunning Techniques (In Progress)
| Date | Paper Title | Links | Tags | 
| :--- | :--- | :--- | :--- |
| 2026-01-15 | **LoRA: Low-Rank Adaptation of Large Language Models** | [PDF](https://arxiv.org/abs/2106.09685) | `LoRA,fine_tuning` |
| 2026-01-16 | **QLoRA: Efficient Finetuning of Quantized LLMs** | [PDF](https://arxiv.org/abs/2305.14314) | `QLoRA,Quantization,fine_tuning` |

ğŸ‘‰ *Check my full notes: [AI-Paper-Notes](https://github.com/MengzhongRe/AI-papers-notes)*

---

<div align="center">
  <img src="https://github-readme-stats.vercel.app/api?username=MengzhongRe&show_icons=true&theme=radical&hide_border=true" height="160" />
</div>
